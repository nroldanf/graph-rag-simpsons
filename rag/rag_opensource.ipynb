{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433c5c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import faiss\n",
    "import openai\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import re\n",
    "import zipfile\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28392e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bae67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_openai_client():\n",
    "    \"\"\"Setup OpenAI client with secure API key handling\"\"\"\n",
    "    # Try multiple methods to get the API key\n",
    "    api_key = None\n",
    "    \n",
    "    # Method 1: Environment variable (recommended)\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    \n",
    "    # Method 2: From .env file (if not found in environment)\n",
    "    if not api_key:\n",
    "        try:\n",
    "            from dotenv import load_dotenv\n",
    "            load_dotenv()\n",
    "            api_key = os.getenv('OPENAI_API_KEY')\n",
    "        except ImportError:\n",
    "            print(\"python-dotenv not installed. Install with: pip install python-dotenv\")\n",
    "    \n",
    "    # Method 3: Interactive input (fallback)\n",
    "    if not api_key:\n",
    "        print(\"OpenAI API key not found in environment variables.\")\n",
    "        print(\"You can set it by:\")\n",
    "        print(\"1. Creating a .env file with: OPENAI_API_KEY=your_key_here\")\n",
    "        print(\"2. Setting environment variable: export OPENAI_API_KEY=your_key_here\")\n",
    "        print(\"3. Enter it now (not recommended for production):\")\n",
    "        api_key = input(\"Enter your OpenAI API key: \").strip()\n",
    "    \n",
    "    if not api_key:\n",
    "        raise ValueError(\"OpenAI API key is required. Please set OPENAI_API_KEY environment variable.\")\n",
    "    \n",
    "    # Set the API key\n",
    "    openai.api_key = api_key\n",
    "    print(\"OpenAI API key configured successfully!\")\n",
    "    \n",
    "    return api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66e298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip_file(zip_path: str, extract_to: str = \"data/extracted\"):\n",
    "    \"\"\"Extract the Simpsons zip file\"\"\"\n",
    "    if not os.path.exists(extract_to):\n",
    "        os.makedirs(extract_to)\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    \n",
    "    print(f\"Extracted {zip_path} to {extract_to}\")\n",
    "    return extract_to\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07020a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_simpsons_data():\n",
    "    \"\"\"Load all Simpsons CSV files into dataframes\"\"\"\n",
    "    zip_path = \"data/thesimpsons.zip\"\n",
    "    extract_path = \"data/extracted\"\n",
    "    \n",
    "    # Extract zip file if it hasn't been extracted yet\n",
    "    if not os.path.exists(extract_path):\n",
    "        extract_zip_file(zip_path, extract_path)\n",
    "    \n",
    "    # Find CSV files in extracted folder\n",
    "    csv_files = {}\n",
    "    for root, dirs, files in os.walk(extract_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                csv_files[file] = os.path.join(root, file)\n",
    "    \n",
    "    # Load the CSV files\n",
    "    characters_df = pd.read_csv(csv_files['simpsons_characters.csv'])\n",
    "    episodes_df = pd.read_csv(csv_files['simpsons_episodes.csv'])\n",
    "    locations_df = pd.read_csv(csv_files['simpsons_locations.csv'])\n",
    "    script_lines_df = pd.read_csv(csv_files['simpsons_script_lines.csv'])\n",
    "    \n",
    "    print(f\"Loaded {len(characters_df)} characters, {len(episodes_df)} episodes, {len(locations_df)} locations, {len(script_lines_df)} script lines\")\n",
    "    \n",
    "    return characters_df, episodes_df, locations_df, script_lines_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975e47a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_episode_documents(episodes_df: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"Create structured documents from episodes data\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for _, episode in episodes_df.iterrows():\n",
    "        # Create comprehensive episode description\n",
    "        content = f\"\"\"\n",
    "        Episode: {episode['title']}\n",
    "        Season: {episode['season']}\n",
    "        Episode Number: {episode['number_in_season']}\n",
    "        Series Number: {episode['number_in_series']}\n",
    "        Air Date: {episode['original_air_date']}\n",
    "        IMDB Rating: {episode['imdb_rating']}\n",
    "        US Viewers: {episode['us_viewers_in_millions']} million\n",
    "        Production Code: {episode['production_code']}\n",
    "        \"\"\"\n",
    "        \n",
    "        documents.append({\n",
    "            'content': content.strip(),\n",
    "            'metadata': {\n",
    "                'type': 'episode',\n",
    "                'episode_id': episode['id'],\n",
    "                'title': episode['title'],\n",
    "                'season': episode['season'],\n",
    "                'imdb_rating': episode['imdb_rating'],\n",
    "                'air_date': episode['original_air_date']\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f56f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_character_documents(characters_df: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"Create structured documents from characters data\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for _, character in characters_df.iterrows():\n",
    "        content = f\"\"\"\n",
    "        Character: {character['name']}\n",
    "        Gender: {character['gender']}\n",
    "        Normalized Name: {character['normalized_name']}\n",
    "        \"\"\"\n",
    "        \n",
    "        documents.append({\n",
    "            'content': content.strip(),\n",
    "            'metadata': {\n",
    "                'type': 'character',\n",
    "                'character_id': character['id'],\n",
    "                'name': character['name'],\n",
    "                'gender': character['gender']\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7148953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_location_documents(locations_df: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"Create structured documents from locations data\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for _, location in locations_df.iterrows():\n",
    "        content = f\"\"\"\n",
    "        Location: {location['name']}\n",
    "        Normalized Name: {location['normalized_name']}\n",
    "        \"\"\"\n",
    "        \n",
    "        documents.append({\n",
    "            'content': content.strip(),\n",
    "            'metadata': {\n",
    "                'type': 'location',\n",
    "                'location_id': location['id'],\n",
    "                'name': location['name']\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666eb35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_script_documents(script_lines_df: pd.DataFrame, episodes_df: pd.DataFrame, \n",
    "                          characters_df: pd.DataFrame, locations_df: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"Create structured documents from script lines with context\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Create lookup dictionaries for faster access\n",
    "    episode_lookup = episodes_df.set_index('id')['title'].to_dict()\n",
    "    character_lookup = characters_df.set_index('id')['name'].to_dict()\n",
    "    location_lookup = locations_df.set_index('id')['name'].to_dict()\n",
    "    \n",
    "    # Group script lines by episode for context\n",
    "    grouped_scripts = script_lines_df.groupby('episode_id')\n",
    "    \n",
    "    for episode_id, episode_lines in grouped_scripts:\n",
    "        episode_title = episode_lookup.get(episode_id, f\"Episode {episode_id}\")\n",
    "        \n",
    "        # Combine multiple lines for richer context\n",
    "        episode_script = []\n",
    "        for _, line in episode_lines.iterrows():\n",
    "            if pd.notna(line['spoken_words']) and line['spoken_words'].strip():\n",
    "                character_name = character_lookup.get(int(line['character_id']), 'Unknown') if pd.notna(line['character_id']) else 'Unknown'\n",
    "                location_name = location_lookup.get(line['location_id'], 'Unknown') if pd.notna(line['location_id']) else 'Unknown'\n",
    "                \n",
    "                script_text = f\"{character_name}: {line['spoken_words']}\"\n",
    "                if location_name != 'Unknown':\n",
    "                    script_text += f\" [Location: {location_name}]\"\n",
    "                \n",
    "                episode_script.append(script_text)\n",
    "        \n",
    "        # Join all lines for the episode\n",
    "        full_script = \"\\n\".join(episode_script)\n",
    "        \n",
    "        if full_script.strip():\n",
    "            documents.append({\n",
    "                'content': f\"Episode: {episode_title}\\n\\nScript:\\n{full_script}\",\n",
    "                'metadata': {\n",
    "                    'type': 'script',\n",
    "                    'episode_id': episode_id,\n",
    "                    'episode_title': episode_title,\n",
    "                    'line_count': len(episode_script)\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879a0ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Split documents using RecursiveCharacterTextSplitter\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \":\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    split_docs = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        chunks = text_splitter.split_text(doc['content'])\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            split_docs.append({\n",
    "                'content': chunk,\n",
    "                'metadata': {\n",
    "                    **doc['metadata'],\n",
    "                    'chunk_id': i,\n",
    "                    'total_chunks': len(chunks)\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7b5be4",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71405ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Split documents using RecursiveCharacterTextSplitter\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \":\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    split_docs = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        chunks = text_splitter.split_text(doc['content'])\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            split_docs.append({\n",
    "                'content': chunk,\n",
    "                'metadata': {\n",
    "                    **doc['metadata'],\n",
    "                    'chunk_id': i,\n",
    "                    'total_chunks': len(chunks)\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7d4c58",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2886cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(documents: List[Dict]) -> tuple:\n",
    "    \"\"\"Create embeddings for all documents\"\"\"\n",
    "    texts = [doc['content'] for doc in documents]\n",
    "    embeddings = embedding_model.encode(texts, convert_to_tensor=False)\n",
    "    \n",
    "    return embeddings, texts, [doc['metadata'] for doc in documents]\n",
    "\n",
    "\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(chunk_df[\"chunk\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "faiss.write_index(index, \"simpsons_faiss.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf69c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faiss_index(embeddings: np.ndarray) -> faiss.Index:\n",
    "    \"\"\"Build FAISS index for similarity search\"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
    "    \n",
    "    # Normalize embeddings for cosine similarity\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings.astype('float32'))\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f03b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_documents(query: str, index: faiss.Index, texts: List[str], \n",
    "                           metadata: List[Dict], k: int = 5) -> List[Dict]:\n",
    "    \"\"\"Search for similar documents using FAISS\"\"\"\n",
    "    query_embedding = embedding_model.encode([query], convert_to_tensor=False)\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    scores, indices = index.search(query_embedding.astype('float32'), k)\n",
    "    \n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        results.append({\n",
    "            'content': texts[idx],\n",
    "            'metadata': metadata[idx],\n",
    "            'score': float(scores[0][i])\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d743d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query: str, context_docs: List[Dict]) -> str:\n",
    "    \"\"\"Generate response using OpenAI with retrieved context\"\"\"\n",
    "    # Prepare context from retrieved documents\n",
    "    context = \"\"\n",
    "    for doc in context_docs:\n",
    "        doc_type = doc['metadata']['type']\n",
    "        context += f\"[{doc_type.upper()}] {doc['content']}\\n\\n\"\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are a knowledgeable assistant about The Simpsons TV show. Use the following context to answer the user's question.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {query}\n",
    "    \n",
    "    Please provide a comprehensive answer based on the context provided. If the context doesn't contain enough information to fully answer the question, mention what information is available and what might be missing.\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant specializing in The Simpsons TV show.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16403ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_simpsons_rag():\n",
    "    \"\"\"Main function to set up the RAG system\"\"\"\n",
    "    print(\"Setting up OpenAI client...\")\n",
    "    setup_openai_client()\n",
    "    \n",
    "    print(\"Loading Simpsons data...\")\n",
    "    characters_df, episodes_df, locations_df, script_lines_df = load_simpsons_data()\n",
    "    \n",
    "    print(\"Creating documents...\")\n",
    "    episode_docs = create_episode_documents(episodes_df)\n",
    "    character_docs = create_character_documents(characters_df)\n",
    "    location_docs = create_location_documents(locations_df)\n",
    "    script_docs = create_script_documents(script_lines_df, episodes_df, characters_df, locations_df)\n",
    "    \n",
    "    # Combine all documents\n",
    "    all_documents = episode_docs + character_docs + location_docs + script_docs\n",
    "    \n",
    "    print(f\"Total documents before splitting: {len(all_documents)}\")\n",
    "    \n",
    "    print(\"Splitting documents...\")\n",
    "    split_docs = split_documents(all_documents)\n",
    "    \n",
    "    print(f\"Total document chunks: {len(split_docs)}\")\n",
    "    \n",
    "    print(\"Creating embeddings...\")\n",
    "    embeddings, texts, metadata = create_embeddings(split_docs)\n",
    "    \n",
    "    print(\"Building FAISS index...\")\n",
    "    index = build_faiss_index(embeddings)\n",
    "    \n",
    "    print(\"RAG system setup complete!\")\n",
    "    \n",
    "    return index, texts, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26b129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_simpsons_rag(query: str, index: faiss.Index, texts: List[str], metadata: List[Dict]) -> str:\n",
    "    \"\"\"Query the Simpsons RAG system\"\"\"\n",
    "    print(f\"Searching for: {query}\")\n",
    "    \n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = search_similar_documents(query, index, texts, metadata, k=5)\n",
    "    \n",
    "    print(f\"Found {len(relevant_docs)} relevant documents\")\n",
    "    \n",
    "    # Generate response\n",
    "    response = generate_response(query, relevant_docs)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8fe789",
   "metadata": {},
   "outputs": [],
   "source": [
    "index, texts, metadata = setup_simpsons_rag()\n",
    "    \n",
    "# Example queries\n",
    "sample_queries = [\n",
    "    \"Which episode has the most lines by Lisa?\",\n",
    "    \"In what season did Milhouse appear most?\",\n",
    "    \"Which characters were in the same location most often as Mr. Burns?\",\n",
    "]\n",
    "\n",
    "for query in sample_queries:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    response = query_simpsons_rag(query, index, texts, metadata)\n",
    "    print(f\"Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
